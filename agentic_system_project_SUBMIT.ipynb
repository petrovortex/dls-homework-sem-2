{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK99BxSO2lSqO1+A5RXHlh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petrovortex/dls-homework-sem-2/blob/main/agentic_system_project_SUBMIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **README**"
      ],
      "metadata": {
        "id": "Z2d4ige257Vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements an **Agentic AI system** designed to estimate the **relevance of Map POIs** (Points of Interest) to user queries using Large Language Models.\n",
        "\n",
        "---\n",
        "\n",
        "### **Input Data:**\n",
        "Training (35k rows) and validation (570 rows) datasets containing:\n",
        "1. User Query `Text`: The raw search request.\n",
        "2. POI Attributes (6 cols): metadata including `name`, `address`, and other descriptions.\n",
        "3. Ground Truth: The `relevance` score of the POI to the query.\n",
        "\n",
        "---\n",
        "\n",
        "### **Base Solution:**\n",
        "\n",
        "The baseline approach utilizes a direct, **single-pass LLM call** with a generic **Zero-shot prompt**. This serves as a benchmark for performance without agentic capabilities.\n",
        "\n",
        "---\n",
        "\n",
        "### **Best Solution:**\n",
        "\n",
        "To overcome the limitations of the baseline, the following architectural improvements were implemented:\n",
        "\n",
        "1.  **RAG System (Knowledge Base):** Construction of a vector-based Knowledge Base from the training dataset to enable **Dynamic Few-Shot Prompting**. This allows the model to take into account labeling patterns from similar historical cases.\n",
        "2.  **Conditional Two-Stage Inference:** If the first model detects ambiguity (needs_search), it triggers a second inference step. Crucially, this step replaces the Knowledge Base examples (which lack useful signal for these specific edge cases) with external search results to make a final decision.\n",
        "3.  **External Search Tool:** Integration of Tavily API to fetch real-time verification data. This tool is invoked only when the primary model lacks confidence, providing the necessary context to resolve the uncertainty.\n",
        "\n",
        "---\n",
        "\n",
        "## **Results:**\n",
        "\n",
        "The notebook concludes with a quantitative assessment of the impact of the listed architectural improvements.\n",
        "\n",
        "---\n",
        "\n",
        "## **Tech Stack:**\n",
        "\n",
        "*   **LLM Orchestration & Routing:**\n",
        "    *   **LiteLLM (Router):** Implements usage-based routing and a fallback mechanism across multiple API endpoints (via OpenRouter) to ensure high availability and the price-less nature of LLMs.\n",
        "    *   **Pydantic:** Used for structured Outputs and rigorous data validation of LLM responses (JSON schema enforcement).\n",
        "*   **Knowledge base components:**\n",
        "    *   **FAISS:** Vector database for efficient similarity search.\n",
        "    *   **Sentence-Transformers (`multilingual-e5-large`):** A high-performance industry-standard model for cross-lingual embeddings\n",
        "*   **Observability & Tracing:**\n",
        "    *   **Opik (Comet):** Comprehensive agent tracing and evaluation platform. Used to monitor spans, track multi-turn reasoning, and perform quantitative evaluation of the agent's performance.\n",
        "*   **Search engine:**\n",
        "    *   **Tavily AI:** Providing clean real-time external context for ambiguous POI queries.\n"
      ],
      "metadata": {
        "id": "SWPlLGwLz53y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0. Data loading**"
      ],
      "metadata": {
        "id": "shNA5Libzi3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZlYErLJ9Rgy",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas openai pydantic requests tqdm litellm opik scikit-learn sentence-transformers faiss-cpu tavily-python razdel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from urllib.parse import urlencode\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def download_file_from_yadisk(public_key: str):\n",
        "    base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
        "    final_url = base_url + urlencode(dict(public_key=public_key))\n",
        "    response = requests.get(final_url)\n",
        "    download_url = response.json()['href']\n",
        "\n",
        "    download_response = requests.get(download_url)\n",
        "    return io.BytesIO(download_response.content)\n",
        "\n",
        "TEST_PUBLIC_LINK = \"https://disk.360.yandex.ru/d/aCpPMD--Yi_y5g\"\n",
        "TRAIN_PUBLIC_LINK = \"https://disk.360.yandex.ru/d/Y4HNAcJh6_cNog\"\n",
        "\n",
        "try:\n",
        "    test_file_content = download_file_from_yadisk(TEST_PUBLIC_LINK)\n",
        "    train_file_content = download_file_from_yadisk(TRAIN_PUBLIC_LINK)\n",
        "\n",
        "    df_test = pd.read_json(test_file_content, lines=True)\n",
        "    df_train = pd.read_json(train_file_content, lines=True)\n",
        "\n",
        "    print(f\"Test dataset loaded. Shape: {df_test.shape}\")\n",
        "    print(f\"Train dataset loaded. Shape: {df_train.shape}\")\n",
        "    print(\"Columns test:\", df_test.columns.tolist())\n",
        "    print(\"Columns train:\", df_train.columns.tolist())\n",
        "except Exception as e:\n",
        "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDNT_fzN95zn",
        "outputId": "0aa963f4-ac7f-4e60-afa0-512c580973ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dataset loaded. Shape: (570, 9)\n",
            "Train dataset loaded. Shape: (35094, 9)\n",
            "Columns test: ['Text', 'address', 'name', 'normalized_main_rubric_name_ru', 'permalink', 'prices_summarized', 'relevance', 'reviews_summarized', 'relevance_new']\n",
            "Columns train: ['Text', 'address', 'name', 'normalized_main_rubric_name_ru', 'permalink', 'prices_summarized', 'relevance', 'reviews_summarized', 'relevance_new']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_kb, df_val = train_test_split(df_train, test_size=0.05, random_state=42, stratify=df_train['relevance_new'])"
      ],
      "metadata": {
        "id": "7FtKVZKQTH3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Imports and configs**"
      ],
      "metadata": {
        "id": "8Ni-Z66h0T84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Optional, Dict, Any, Union\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "import litellm\n",
        "from litellm import Router\n",
        "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
        "\n",
        "from opik import track\n",
        "from opik.opik_context import get_current_span_data, update_current_span\n",
        "from litellm.integrations.opik.opik import OpikLogger\n",
        "\n",
        "from tavily import TavilyClient\n",
        "\n",
        "from razdel import sentenize\n",
        "import re\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "os.environ[\"OPIK_API_KEY\"] = \"...\"\n",
        "os.environ[\"OPIK_WORKSPACE\"] = \"default\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"tvly-dev-...\"\n",
        "\n",
        "opik_logger = OpikLogger()\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "API_KEYS = [\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\"\n",
        "]\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    #\"mimo-v2-flash\": \"openrouter/xiaomi/mimo-v2-flash:free\",\n",
        "    \"glm-4.5-air\": \"openrouter/z-ai/glm-4.5-air:free\",\n",
        "    \"deepseek-r1\": \"openrouter/deepseek/deepseek-r1-0528:free\",\n",
        "    \"trinity\": \"openrouter/arcee-ai/trinity-large-preview:free\",\n",
        "    \"nemotron\": \"openrouter/nvidia/nemotron-3-nano-30b-a3b:free\",\n",
        "}\n",
        "\n",
        "model_list = []\n",
        "\n",
        "for alias, model_id in MODELS_CONFIG.items():\n",
        "    for key in API_KEYS:\n",
        "        model_list.append({\n",
        "            \"model_name\": alias,\n",
        "            \"litellm_params\": {\n",
        "                \"model\": model_id,\n",
        "                \"api_key\": key\n",
        "            }\n",
        "        })\n",
        "\n",
        "llm_router = Router(\n",
        "    model_list=model_list,\n",
        "    routing_strategy=\"usage-based-routing\",\n",
        "    num_retries=len(API_KEYS)-1,\n",
        "    allowed_fails=1,\n",
        "    cooldown_time=600\n",
        ")\n",
        "\n",
        "class BaseOutput(BaseModel):\n",
        "    reasoning: str\n",
        "    relevance: float\n",
        "\n",
        "    @field_validator('relevance')\n",
        "    @classmethod\n",
        "    def snap_relevance(cls, v: float) -> float:\n",
        "        allowed = [0.0, 0.1, 1.0]\n",
        "        return min(allowed, key=lambda x: abs(x - v))\n",
        "\n",
        "class Tier1Output(BaseOutput):\n",
        "    needs_search: bool\n",
        "    search_query: Optional[str] = None\n",
        "\n",
        "class Tier2Output(BaseOutput):\n",
        "    reasoning: str = Field(description=\"Final detailed reasoning\")\n",
        "\n",
        "def clean_json_content(content: str) -> str:\n",
        "    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)\n",
        "\n",
        "    start_index = content.find('{')\n",
        "    end_index = content.rfind('}')\n",
        "\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        return content[start_index : end_index + 1]\n",
        "\n",
        "    return content.strip()"
      ],
      "metadata": {
        "id": "mfqTRZhMjynb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_model(model_alias):\n",
        "    \"\"\"\n",
        "    check if model is alive\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Answer 'pong' in json field 'answer'\"},\n",
        "        {\"role\": \"user\", \"content\": \"ping\"}\n",
        "    ]\n",
        "    try:\n",
        "        response = llm_router.completion(\n",
        "            model=model_alias,\n",
        "            messages=messages,\n",
        "            response_format={ \"type\": \"json_object\" }\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        if model_alias == 'deepseek-r1':\n",
        "            content = clean_json_content(content)\n",
        "        print(content)\n",
        "    except Exception as e:\n",
        "        print(f\"LLM Call: {e}\")"
      ],
      "metadata": {
        "id": "PzvWm_Nshkak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_model('deepseek-r1')"
      ],
      "metadata": {
        "id": "oqnP2GBijI99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9069c6d2-4b65-412e-c9fd-7c438a6a4ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"answer\": \"pong\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_results = tavily_client.search(\n",
        "                \"–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –°–æ–≤–µ—Ç—Å–∫–∞—è –∞–ø—Ç–µ–∫–∞ –ú–∞—Ö–∞—á–∫–∞–ª–∞\",\n",
        "                search_depth=\"basic\",\n",
        "                include_answer=True,\n",
        "                max_results=5\n",
        "            )"
      ],
      "metadata": {
        "id": "vqQ7M2Qju2IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_results"
      ],
      "metadata": {
        "id": "QUqKkJMjvN4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Agent components**"
      ],
      "metadata": {
        "id": "LhUeislA0znC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Knowledge Base"
      ],
      "metadata": {
        "id": "9H6umzfB1XYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_chunks(model, query, chunks, k=3):\n",
        "    if not chunks or len(chunks) <= k:\n",
        "        return chunks\n",
        "\n",
        "    query_embedding = model.encode([f\"query: {query}\"], normalize_embeddings=True)\n",
        "    chunk_embeddings = model.encode([f\"passage: {c}\" for c in chunks], normalize_embeddings=True)\n",
        "\n",
        "    cos_scores = util.cos_sim(query_embedding, chunk_embeddings)[0]\n",
        "\n",
        "    k_actual = min(k, len(chunks))\n",
        "    top_results = torch.topk(cos_scores, k=k_actual)\n",
        "    top_indices = top_results.indices.tolist()\n",
        "\n",
        "    return [chunks[i] for i in top_indices]"
      ],
      "metadata": {
        "id": "bi3yPWxunsAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeBase:\n",
        "    def __init__(self, dataframe, model_name='intfloat/multilingual-e5-large', index_path=\"kb_index\"):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.model_name = model_name\n",
        "        self.index_path_faiss = f\"{index_path}.faiss\"\n",
        "        self.index_path_meta = f\"{index_path}.pkl\"\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Loading embedding model on {self.device}...\")\n",
        "        self.model = SentenceTransformer(model_name, device=self.device)\n",
        "\n",
        "        self.index = None\n",
        "\n",
        "        if os.path.exists(self.index_path_faiss) and os.path.exists(self.index_path_meta):\n",
        "            self.load()\n",
        "        else:\n",
        "            self.build_index()\n",
        "\n",
        "    def _row_to_text(self, row, is_query):\n",
        "        prefix = \"query: \" if is_query else \"passage: \"\n",
        "        content = f\"User request: {row['Text']} | Name: {row['name']} | Address: {row['address']} | Rubric: {row['normalized_main_rubric_name_ru']}\"\n",
        "        return prefix + content\n",
        "\n",
        "    def build_index(self, batch_size=64):\n",
        "        print(\"Building Vector Index...\")\n",
        "        sentences = self.df.apply(lambda x: self._row_to_text(x, is_query=False), axis=1).tolist()\n",
        "\n",
        "        embeddings = self.model.encode(\n",
        "            sentences,\n",
        "            batch_size=batch_size,\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True\n",
        "        )\n",
        "\n",
        "        d = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(d)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "        print(f\"Index built. Vectors: {self.index.ntotal}\")\n",
        "        self.save()\n",
        "\n",
        "    @track(name=\"vector_search\")\n",
        "    def search(self, row, k=3):\n",
        "        query_text = self._row_to_text(row, is_query=True)\n",
        "        query_vec = self.model.encode([query_text], convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "        distances, indices = self.index.search(query_vec, k + 1)\n",
        "\n",
        "        results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if i >= k: break\n",
        "\n",
        "            match_row = self.df.iloc[idx]\n",
        "            name_chunks = [n.strip() for n in str(match_row['name']).split(';') if n.strip()]\n",
        "\n",
        "            top_names = get_top_k_chunks(\n",
        "                self.model,\n",
        "                query=match_row['Text'],\n",
        "                chunks=name_chunks,\n",
        "                k=2\n",
        "            )\n",
        "\n",
        "            results.append({\n",
        "                \"text\": match_row['Text'],\n",
        "                \"name\": \" | \".join(top_names),\n",
        "                \"relevance\": match_row['relevance'],\n",
        "                \"distance\": float(distances[0][i])\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    def save(self):\n",
        "        faiss.write_index(self.index, self.index_path_faiss)\n",
        "        with open(self.index_path_meta, \"wb\") as f:\n",
        "            pickle.dump(self.df, f)\n",
        "\n",
        "    def load(self):\n",
        "        self.index = faiss.read_index(self.index_path_faiss)\n",
        "        with open(self.index_path_meta, \"rb\") as f:\n",
        "            self.df = pickle.load(f)"
      ],
      "metadata": {
        "id": "qnWSTT6qHoBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_folder = \"/content/drive/MyDrive/YandexMaps_Agent_Project\"\n",
        "os.makedirs(drive_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "KFuJXjgxfOwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_path_drive = os.path.join(drive_folder, \"kb_index\")\n",
        "kb = KnowledgeBase(df_kb, index_path=index_path_drive)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CE_qXGN1H45l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Prompts"
      ],
      "metadata": {
        "id": "dD3La_LH1ja-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = kb.model"
      ],
      "metadata": {
        "id": "L_1rxvBWMemx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input_context(row: pd.Series, has_reviews=True) -> str:\n",
        "    \"\"\"\n",
        "    Constructs a concise context prompt by semantically filtering POI attributes against the user query.\n",
        "\n",
        "    The function splits composite fields (names, prices, reviews) into chunks and\n",
        "    retains only the top-k segments that are semantically closest to the query (using vector search).\n",
        "    This ensures the context remains focused on relevant details.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row containing POI metadata (Text, name, address, etc.).\n",
        "        has_reviews (bool): Whether to include summarized reviews in the output.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted XML-like string containing the filtered object details.\n",
        "    \"\"\"\n",
        "    user_text = row['Text']\n",
        "\n",
        "    name_chunks = [n.strip() for n in str(row['name']).split(';') if n.strip()]\n",
        "    top_names = get_top_k_chunks(vectorizer, user_text, name_chunks, k=3)\n",
        "    name_str = \" | \".join(top_names)\n",
        "\n",
        "    prices_str = \"No data\"\n",
        "    if pd.notna(row['prices_summarized']):\n",
        "        parts = re.split(r'[|\\n]', str(row['prices_summarized']))\n",
        "        parts = [p.strip() for p in parts if p.strip()]\n",
        "        summary = parts[0].strip()\n",
        "        chunks = [p.strip() for p in parts[1:] if p.strip()]\n",
        "        top_chunks = get_top_k_chunks(vectorizer, user_text, chunks, k=3)\n",
        "        prices_str = f\"{summary} [Details: {' | '.join(top_chunks)}]\" if top_chunks else summary\n",
        "\n",
        "    reviews_str = \"No data\"\n",
        "    if pd.notna(row['reviews_summarized']) and has_reviews:\n",
        "        parts = re.split(r'[|\\n]', str(row['reviews_summarized']))\n",
        "        parts = [r.strip() for r in parts if r.strip()]\n",
        "        summary = parts[0].strip()\n",
        "        chunks = [r.strip() for r in parts[1:] if r.strip()]\n",
        "        top_chunks = get_top_k_chunks(vectorizer, user_text, chunks, k=3)\n",
        "        reviews_str = f\"{summary} [Key snippets: {' | '.join(top_chunks)}]\" if top_chunks else summary\n",
        "\n",
        "    return f\"\"\"\n",
        "<context>\n",
        "  <user_query>\"{user_text}\"</user_query>\n",
        "\n",
        "  <object_details>\n",
        "    - RUBRIC: {row['normalized_main_rubric_name_ru']}\n",
        "    - NAME: {name_str}\n",
        "    - ADDRESS: {row['address']}\n",
        "    - PRICE_INFO: {prices_str}\n",
        "    - REVIEW_HIGHLIGHTS: {reviews_str}\n",
        "  </object_details>\n",
        "</context>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JaeXyBD_MbbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_instructions():\n",
        "    return \"\"\"\n",
        "You are a search relevance expert. Your goal is to assess if a map object satisfies a user query.\n",
        "\n",
        "### DATA STRUCTURE INTERPRETATION:\n",
        "- **SUMMARY (General Info):** This is a broad overview. If it says \"The shop sells electronics\", it's a general truth.\n",
        "- **HIGHLIGHTS/DETAILS (Evidence):** These are specific snippets selected as most relevant to the query. They are NOT an exhaustive list.\n",
        "- **PRUNED NAMES:** You only see names most similar to the query.\n",
        "\n",
        "### VALID RELEVANCE VALUES:\n",
        "- 1.0: Perfect match. The object definitely provides what the user wants.\n",
        "- 0.0: Irrelevant. Wrong category, closed, or completely unrelated.\n",
        "- 0.1: Partial/Unsure. The object might be relevant, but specific IMPORTANT constraints (price, specific service, rare item) are not explicitly confirmed in the provided snippets.\n",
        "\n",
        "### INSTRUCTIONS:\n",
        "1. Think step-by-step. Detect the **USER INTENT**: Is the user looking for a specific **ITEM/SERVICE** (e.g., \"buy pills\", \"sauna\") or a specific **VENUE TYPE** (e.g., \"Pharmacy\", \"Recreation Base\")?\n",
        "2. **IF USER WANTS A VENUE TYPE (Category Search):**\n",
        "   - **RUBRIC PRIORITY:** The object's `Rubric` must semantically match the requested venue type.\n",
        "   - **MISMATCH PENALTY:** If the user asks for \"Category A\" (e.g., \"Holiday Center\") and the object is \"Category B\" (e.g., \"Sports Camp\"), the relevance is likely **0.0**, even if they share some features (like saunas or beds).\n",
        "   - *Exception:* Only give 0.1 or 1.0 if Category B is relative (like \"–∫–∞—Ñ–µ\" and \"—Ä–µ—Å—Ç–æ—Ä–∞–Ω\") or a direct sub-type or synonym of Category A.\n",
        "3. **IF USER WANTS AN ITEM/SERVICE:**\n",
        "   - **CATEGORY LOGIC:** If the user asks for a COMMON item (e.g., \"aspirin\") and the object is a standard provider (Pharmacy), **RELEVANCE IS 1.0**.\n",
        "   - **PARTIAL INVENTORY (The \"Open List\" Rule):**\n",
        "     - Treat 'Prices' and 'Description' as **incomplete examples**, not a full catalog.\n",
        "     - If the object sells \"Bags\" but lists only \"Wallets\", assume it MIGHT sell \"Suitcases\".\n",
        "     - **DO NOT DOWNGRADE TO 0.0** just because a specific item or brand is missing from the text description, unless the category makes it impossible (e.g., asking for \"Suitcase\" in a \"Bakery\").\n",
        "     - **Verdict:** If Category matches but Item/Brand is unconfirmed -> Relevance is **0.1**.\n",
        "   - Use 0.1 for RARE/SPECIFIC items where availability is truly unknown.\n",
        "4. Pay attention to \"Hard Constraints\" (open now, free wifi) in user request.\n",
        "5. QUERY RECOVERY (Typos & Layout Errors):\n",
        "   - DETECT NOISE: Check if the query contains obvious typos (e.g., \"–∫—É–ø–∏—Ç—å –º–∞—à–∏–Ω–∏—Å—Ç\" instead of \"–∫—É–ø–∏—Ç—å –º–∞—à–∏–Ω—É\") or wrong keyboard layout patterns (e.g., gibberish text that maps to meaningful words in another language/layout).\n",
        "   - RECONSTRUCT INTENT: If the literal query is nonsensical but a highly probable correction exists, evaluate the object based on the CORRECTED query.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "F-39gkmGxlD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_format(schema):\n",
        "    return f\"\"\"\n",
        "### **OUTPUT FORMAT:**\n",
        "Return strictly ONLY a valid JSON object (without thinking tags) matching this schema.\n",
        "{schema}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VWauqMD1ymKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rag_instructions():\n",
        "    return \"\"\"\n",
        "### HANDLING HISTORICAL EXAMPLES:\n",
        "- Use provided \"SIMILAR HISTORICAL CASES\" to calibrate your judgment (see how strict the scoring should be for this category).\n",
        "- **WARNING:** Historical labels can be noisy or based on outdated info.\n",
        "- If a historical case contradicts the current data or common sense, prioritize your own analysis of the current object.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RP6lPLpRxFMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_search_instructions():\n",
        "    return \"\"\"\n",
        "### UNCERTAINTY & SEARCH:\n",
        "- If your analysis leads to **0.1**, you MUST set `needs_search` to `true`.\n",
        "- **SEARCH QUERY RULES:**\n",
        "  1. Language: Russian.\n",
        "  2. Format: [Missing feature] + [Object Name] + [Address or City].\n",
        "  3. Example: \"–Ω–∞–ª–∏—á–∏–µ –±–∞—Å—Å–µ–π–Ω–∞ –≤ –ª–∞–≥–µ—Ä–µ –û—Å—Ç—Ä–æ–≤ –¥–µ—Ç—Å—Ç–≤–∞ –¢—é–º–µ–Ω—å\" or \"–º–µ–Ω—é –∫–∞—Ñ–µ –†–æ–º–∞—à–∫–∞ –Ω–∞ –õ–µ–Ω–∏–Ω–∞ 10\".\n",
        "- If relevance is definitively 0.0 or 1.0, set `needs_search` to `false`.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gfIwUG043aZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_system_prompt(is_tier2=False, use_rag=False, use_search=False) -> str:\n",
        "    \"\"\"\n",
        "    Collects system prompt:\n",
        "    1. Base instructions\n",
        "    2. RAG instructions (optional)\n",
        "    3. Search instructions (optional)\n",
        "    4. Output format instructions\n",
        "    \"\"\"\n",
        "    system_prompt = get_base_instructions()\n",
        "    if use_rag:\n",
        "        system_prompt += get_rag_instructions()\n",
        "    if use_search:\n",
        "        system_prompt += get_search_instructions()\n",
        "    if is_tier2:\n",
        "        system_prompt += get_output_format(Tier2Output.model_json_schema())\n",
        "    else:\n",
        "        system_prompt += get_output_format(Tier1Output.model_json_schema())\n",
        "    return system_prompt"
      ],
      "metadata": {
        "id": "t4tJj5vK57Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_rag_context(examples: list) -> str:\n",
        "    if not examples:\n",
        "        return \"No similar examples found.\"\n",
        "\n",
        "    text = \"\"\n",
        "    for i, ex in enumerate(examples):\n",
        "        text += f\"\"\"---\n",
        "[Historical Example {i+1}]\n",
        "Query: \"{ex['text']}\"\n",
        "Object: \"{ex['name']}\"\n",
        "Relevance: {ex['relevance']}\n",
        "---\"\"\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "03UlbrKqCMxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_prompt(\n",
        "        context: str,\n",
        "        examples: Optional[str] = None,\n",
        "        search_results: Optional[str] = None\n",
        "    ):\n",
        "    sections = []\n",
        "\n",
        "    sections.append(context)\n",
        "\n",
        "    if examples:\n",
        "        sections.append(f\"<historical_examples>\\n{examples}\\n</historical_examples>\")\n",
        "\n",
        "    if search_results:\n",
        "        sections.append(f\"<web_search_results>\\n{search_results}\\n</web_search_results>\")\n",
        "\n",
        "    sections.append(\"Analyze the data above and provide your assessment in JSON format.\")\n",
        "\n",
        "    return \"\\n\\n\".join(sections)"
      ],
      "metadata": {
        "id": "5k9Wy17HCPA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Agent class"
      ],
      "metadata": {
        "id": "H0K8GyKo1tDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentConfig(BaseModel):\n",
        "    model_alias: str\n",
        "    use_rag: bool = False\n",
        "    use_search: bool = False\n",
        "    name: str  # –ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä \"deepseek_rag\""
      ],
      "metadata": {
        "id": "cYWISiB_SwC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(\n",
        "            self, router: Router,\n",
        "            kb: KnowledgeBase,\n",
        "            search_client: TavilyClient,\n",
        "            config: AgentConfig\n",
        "    ):\n",
        "        self.router = router\n",
        "        self.kb = kb\n",
        "        self.search_client = search_client\n",
        "        self.config = config\n",
        "\n",
        "    @track\n",
        "    def _call_llm(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        response_model: Union[Tier1Output, Tier2Output],\n",
        "        step_name: str,\n",
        "        timeout: int = 180,\n",
        "        max_retries: int = 3\n",
        "    ):\n",
        "        update_current_span(name=step_name)\n",
        "\n",
        "        current_messages = list(messages)\n",
        "\n",
        "        completion_params = {\n",
        "            \"model\": self.config.model_alias,\n",
        "            \"timeout\": timeout,\n",
        "            \"response_format\": response_model\n",
        "        }\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                completion_params[\"messages\"] = current_messages\n",
        "\n",
        "                response = self.router.completion(**completion_params)\n",
        "                content = clean_json_content(response.choices[0].message.content.strip())\n",
        "\n",
        "                return response_model.model_validate_json(content)\n",
        "\n",
        "            except (ValidationError, json.JSONDecodeError) as e:\n",
        "                print(f\"‚ö†Ô∏è JSON Validation Error in {step_name} (Attempt {attempt + 1}/{max_retries}): {e}\")\n",
        "\n",
        "                if attempt == max_retries - 1:\n",
        "                    print(f\"‚ùå Failed to validate JSON after {max_retries} attempts.\")\n",
        "                    return None\n",
        "\n",
        "                current_messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                error_feedback = (\n",
        "                    f\"Your previous response caused a JSON validation error: {str(e)}. \"\n",
        "                    f\"Please fix the JSON structure to match the schema exactly. \"\n",
        "                    f\"Return ONLY the JSON.\"\n",
        "                )\n",
        "                current_messages.append({\"role\": \"user\", \"content\": error_feedback})\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = str(e).lower()\n",
        "\n",
        "                if \"timeout\" in error_msg:\n",
        "                    print(f\"‚è≥ Timeout Error in {step_name} after {timeout}s\")\n",
        "                    return None\n",
        "\n",
        "                print(f\"üõë Critical LLM Error in {step_name}: {e}\")\n",
        "                print(\"Stopping execution safely...\")\n",
        "\n",
        "                raise KeyboardInterrupt\n",
        "\n",
        "    @track(name=\"tavily_search\")\n",
        "    def _execute_search(self, query: str) -> str:\n",
        "        try:\n",
        "            search_answer = self.search_client.search(\n",
        "                query,\n",
        "                include_answer=True,\n",
        "                max_results=5\n",
        "            )['answer']\n",
        "\n",
        "            return search_answer\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Search failed: {e}\"\n",
        "\n",
        "    @track(name=\"relevance_prediction\")\n",
        "    def predict(self, row: pd.Series) -> dict:\n",
        "        tier1_system_prompt = get_system_prompt(\n",
        "            is_tier2=False,\n",
        "            use_rag=self.config.use_rag,\n",
        "            use_search=self.config.use_search\n",
        "        )\n",
        "\n",
        "        context = format_input_context(row)\n",
        "        examples = None\n",
        "        if self.config.use_rag:\n",
        "            examples_list = self.kb.search(row, k=2)\n",
        "            examples = format_rag_context(examples_list)\n",
        "\n",
        "        tier1_user_prompt = get_user_prompt(context, examples)\n",
        "\n",
        "        messages_tier1 = [\n",
        "            {\"role\": \"system\", \"content\": tier1_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": tier1_user_prompt}\n",
        "        ]\n",
        "\n",
        "        tier1_output = self._call_llm(messages_tier1, Tier1Output, step_name=\"tier1_preliminary_analysis\")\n",
        "\n",
        "        trace_log = {\n",
        "            \"tier1_reasoning\": None,\n",
        "            \"tier1_relevance\": None,\n",
        "            \"needs_search\": None,\n",
        "            \"search_query\": None,\n",
        "            \"search_results\": None,\n",
        "            \"tier2_reasoning\": None,\n",
        "            \"tier2_relevance\": None,\n",
        "            \"final_relevance\": None\n",
        "        }\n",
        "\n",
        "        if not tier1_output:\n",
        "            return trace_log\n",
        "\n",
        "        trace_log[\"tier1_reasoning\"] = tier1_output.reasoning\n",
        "        trace_log[\"tier1_relevance\"] = tier1_output.relevance\n",
        "        trace_log[\"needs_search\"] = tier1_output.needs_search\n",
        "        trace_log[\"search_query\"] = tier1_output.search_query\n",
        "        trace_log[\"final_relevance\"] = tier1_output.relevance\n",
        "\n",
        "        should_search = tier1_output.needs_search and self.config.use_search\n",
        "\n",
        "        if should_search and tier1_output.search_query:\n",
        "            search_results = self._execute_search(tier1_output.search_query)\n",
        "            trace_log[\"search_results\"] = search_results\n",
        "\n",
        "            tier2_system_prompt = get_system_prompt(\n",
        "                is_tier2=True,\n",
        "                use_rag=self.config.use_rag,\n",
        "                use_search=False\n",
        "            )\n",
        "\n",
        "            poor_context = format_input_context(row, has_reviews=False)\n",
        "            tier2_user_prompt = get_user_prompt(poor_context, examples, search_results)\n",
        "\n",
        "            messages_tier2 = [\n",
        "                {\"role\": \"system\", \"content\": tier2_system_prompt},\n",
        "                {\"role\": \"user\", \"content\": tier2_user_prompt}\n",
        "            ]\n",
        "\n",
        "            tier2_output = self._call_llm(messages_tier2, Tier2Output, step_name=\"tier2_final_analysis\")\n",
        "\n",
        "            if tier2_output:\n",
        "                trace_log[\"tier2_reasoning\"] = tier2_output.reasoning\n",
        "                trace_log[\"tier2_relevance\"] = tier2_output.relevance\n",
        "                trace_log[\"final_relevance\"] = tier2_output.relevance\n",
        "\n",
        "        return trace_log"
      ],
      "metadata": {
        "id": "arY1XaaQzKPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Evaluation**\n"
      ],
      "metadata": {
        "id": "J0GjDapM2ccB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik.evaluation.metrics.score_result import ScoreResult\n",
        "from opik.evaluation.metrics import Equals, BaseMetric\n",
        "from opik.evaluation import evaluate\n",
        "from opik import Opik\n",
        "\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "sTOmXgxYwU3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(name, df, sample_size=40):\n",
        "    client = Opik()\n",
        "    dataset = client.get_or_create_dataset(name=name)\n",
        "\n",
        "    if dataset.dataset_items_count == None or dataset.dataset_items_count == 0:\n",
        "        sample = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
        "        records = [\n",
        "            {\n",
        "                \"input_data\": row.to_dict(),\n",
        "                \"reference\": str(float(row['relevance_new']))\n",
        "            }\n",
        "            for _, row in sample.iterrows()\n",
        "        ]\n",
        "        dataset.insert(records)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Q-5Z88DR7o0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_task(item, agent):\n",
        "    row = pd.Series(item[\"input_data\"])\n",
        "    trace_log = agent.predict(row)\n",
        "\n",
        "    final_rel = trace_log.get(\"final_relevance\")\n",
        "    output_val = str(float(final_rel)) if final_rel is not None else \"-1.0\"\n",
        "\n",
        "    return {\n",
        "        \"output\": output_val,\n",
        "        \"tier1_relevance\": trace_log.get(\"tier1_relevance\"),\n",
        "        \"tier1_reasoning\": trace_log.get(\"tier1_reasoning\"),\n",
        "        \"search_results\": trace_log.get(\"search_results\"),\n",
        "        \"search_query\": trace_log.get(\"search_query\"),\n",
        "        \"tier2_reasoning\": trace_log.get(\"tier2_reasoning\"),\n",
        "        \"tier2_relevance\": trace_log.get(\"tier2_relevance\")\n",
        "    }"
      ],
      "metadata": {
        "id": "LITJm4zk4GtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyAccuracyMetric(BaseMetric):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"Accuracy\")\n",
        "\n",
        "    def score(self, output, reference, **kwargs):\n",
        "        if output is None:\n",
        "             return ScoreResult(value=-1.0, name=self.name)\n",
        "\n",
        "        try:\n",
        "            matches = float(output) == float(reference)\n",
        "            val = 1.0 if matches else 0.0\n",
        "        except:\n",
        "            val = 0.0\n",
        "\n",
        "        return ScoreResult(value=val, name=self.name)\n",
        "\n",
        "class RelevanceQualityMetric(BaseMetric):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"Soft Match Score\")\n",
        "\n",
        "    def score(self, output, reference, **kwargs):\n",
        "        if output is None:\n",
        "            return ScoreResult(value=-1.0, name=self.name)\n",
        "\n",
        "        if isinstance(output, dict):\n",
        "            pred = output.get(\"output\", -1)\n",
        "        else:\n",
        "            pred = output\n",
        "\n",
        "        pred = float(pred)\n",
        "        target = float(reference)\n",
        "\n",
        "        if pred == target:\n",
        "            val = 1.0\n",
        "        else:\n",
        "            pair = {pred, target}\n",
        "            if pair == {0.0, 1.0}:\n",
        "                val = 0.0\n",
        "            else:\n",
        "                val = 0.5\n",
        "\n",
        "        return ScoreResult(value=val, name=self.name)"
      ],
      "metadata": {
        "id": "Mm_OrC0yIbzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = prepare_dataset(\"Search_Relevance_Benchmark_40\", df_val)\n",
        "\n",
        "model_aliases = [\"glm-4.5-air\", \"trinity\", \"nemotron\"]\n",
        "\n",
        "metrics = [\n",
        "    MyAccuracyMetric(),\n",
        "    RelevanceQualityMetric()\n",
        "]\n",
        "\n",
        "configs = [\n",
        "    {\"use_rag\": True, \"use_search\": True, \"label\": \"full\"},\n",
        "    {\"use_rag\": True, \"use_search\": False, \"label\": \"rag\"},\n",
        "    {\"use_rag\": False, \"use_search\": True, \"label\": \"search\"},\n",
        "    {\"use_rag\": False, \"use_search\": False, \"label\": \"baseline\"}\n",
        "]"
      ],
      "metadata": {
        "id": "GrFol11czpJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in model_aliases:\n",
        "    for cfg in configs:\n",
        "        agent_config = AgentConfig(\n",
        "            model_alias=model,\n",
        "            use_rag=cfg[\"use_rag\"],\n",
        "            use_search=cfg[\"use_search\"],\n",
        "            name=f\"{model}_{cfg['label']}\"\n",
        "        )\n",
        "\n",
        "        agent = Agent(router=llm_router, kb=kb, search_client=tavily_client, config=agent_config)\n",
        "        task = partial(evaluation_task, agent=agent)\n",
        "\n",
        "        evaluate(\n",
        "            dataset=dataset,\n",
        "            task=task,\n",
        "            task_threads=1,\n",
        "            scoring_metrics=metrics,\n",
        "            experiment_name=agent_config.name,\n",
        "            project_name='POI_relevance',\n",
        "        )"
      ],
      "metadata": {
        "id": "q03b0WDo1Q5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Results**"
      ],
      "metadata": {
        "id": "Dla2z1F3nE-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Metrics Definition**\n",
        "To capture a nuanced view of model performance, two metrics were used:\n",
        "1.  **Accuracy:** Exact match between the predicted and ground truth relevance scores.\n",
        "2.  **Soft Match Score:** A weighted metric providing a partial credit (0.5) for \"near-miss\" errors (specifically cases involving the `0.1` relevance class), while penalizing binary flips between `0` and `1`."
      ],
      "metadata": {
        "id": "Hy45SsgtS06Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Component Impact (Ablation Study)**\n",
        "A series of experiments was conducted using **Trinity-Large** and **Nemotron-3-Nano** to evaluate the incremental value of each architectural component.\n",
        "*   **Baseline vs. RAG/Search:** For both models, we observed a consistent upward trend in performance. Adding RAG and Web Search capabilities improved accuracy by **~15-20%** compared to the Zero-shot baseline.\n",
        "*   **Agentic Workflow (Full):** The \"Full\" configuration (System Prompts + RAG + Search + Reflection) yielded the highest scores for these models, peaking at **0.70 Accuracy** for Nemotron-3-Nano."
      ],
      "metadata": {
        "id": "AQ5Xb7wlYUrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Evaluation Results](https://raw.githubusercontent.com/petrovortex/dls-homework-sem-2/main/img/trinity.png)"
      ],
      "metadata": {
        "id": "Dxu0n-9cYWP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Evaluation Results](https://raw.githubusercontent.com/petrovortex/dls-homework-sem-2/main/img/nemotron.png)"
      ],
      "metadata": {
        "id": "0Zx29StBYbGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Scaling & Model Architecture**\n",
        "The impact of model scale and reasoning capabilities was analyzed across different architectures:\n",
        "*   **GLM-4.5-Air:** Demonstrated the highest overall performance with **0.85 Accuracy**. While significantly more accurate, it introduced a latency trade-off, with an average response time of **~30s**.\n",
        "*   **Reasoning Model (DeepSeek-R1):** Despite its reasoning capabilities, DeepSeek-R1 showed lower performance in this specific task. It exhibited high latency (**88s avg.**) and frequently **struggled with JSON schema** enforcement, leading to validation errors.\n",
        "*   **Small Model (GPT-5-Nano):** Showed limited utility for this complex relevance estimation task, significantly underperforming compared to mid-sized alternatives."
      ],
      "metadata": {
        "id": "ThtdNleWYNjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Evaluation Results](https://raw.githubusercontent.com/petrovortex/dls-homework-sem-2/main/img/all_full.png)"
      ],
      "metadata": {
        "id": "OYAlZvCNYpA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Evaluation Results](https://raw.githubusercontent.com/petrovortex/dls-homework-sem-2/main/img/info.png)"
      ],
      "metadata": {
        "id": "ktgoIx2mYuzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Experimental Constraints**\n",
        "*   **Sample Size:** It is important to note that due to latency and rate limits, experiments for **DeepSeek-R1** and **GPT-5-Nano** were conducted on a smaller subset of the validation data.\n",
        "*   **Operational Factors:** Average durations may have been influenced by fluctuating network stability and API response variability during the evaluation process.\n",
        "\n",
        "### **4. Potential Enhancements: Programmatic Prompt Optimization**\n",
        "A planned next step for this project is to migrate the agent logic to the DSPy framework.\n",
        "\n",
        "**Objective**: To move away from manual prompts.\n",
        "\n",
        "According to DSPy's documentation, the system can automatically generate more robust instructions and few-shot examples by learning directly from failure patterns identified in the training dataset.\n",
        "\n",
        "This would allow for a rigorous comparison between human-engineered system prompts and those optimized by DSPy's compiler, potentially leading to higher performance and better generalization across different model scales."
      ],
      "metadata": {
        "id": "ndej2Ag0YQHj"
      }
    }
  ]
}