{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "shNA5Libzi3m",
        "8Ni-Z66h0T84",
        "aYYmtGOJ72Ix",
        "_XVFiL6g1LkU",
        "9H6umzfB1XYE",
        "dD3La_LH1ja-",
        "H0K8GyKo1tDj",
        "J0GjDapM2ccB"
      ],
      "authorship_tag": "ABX9TyMG3vVfTfpVe9f7hhBi2y/6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petrovortex/dls-homework-sem-2/blob/main/agentic_system_project_%5Bpublic%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO-DO:\n",
        "1. (done) Implement search functionality in the BestAgent class.\n",
        "2. (done) Update instructions and output format for the draft (so it will generate a question for Tavily).\n",
        "3. (done) Make the report in run_evaluation more informative.\n",
        "4. Analyze whether adopting agentic frameworks improves code modularity and flexibility compared to the current implementation."
      ],
      "metadata": {
        "id": "kJnYQ8Al-Y83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## README"
      ],
      "metadata": {
        "id": "Z2d4ige257Vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements an **Agentic AI system** designed to estimate the **relevance of Map POIs** (Points of Interest) to user queries using Large Language Models.\n",
        "\n",
        "---\n",
        "\n",
        "**Input Data:** Training (35k rows) and validation (570 rows) datasets containing:\n",
        "1. User Query `Text`: The raw search request.\n",
        "2. POI Attributes (6 cols): metadata including `name`, `address`, and other descriptions.\n",
        "3. Ground Truth: The `relevance` score of the POI to the query.\n",
        "\n",
        "---\n",
        "\n",
        "**Base Solution:**\n",
        "\n",
        "The baseline approach utilizes a direct, **single-pass LLM call** with a generic **Zero-shot prompt** (e.g., *'Determine the relevance of {object} to {query}'*). This serves as a benchmark for performance without agentic capabilities.\n",
        "\n",
        "---\n",
        "\n",
        "**Best Solution:**\n",
        "\n",
        "To overcome the limitations of the baseline, the following architectural improvements were implemented:\n",
        "\n",
        "1.  **Advanced Prompt Engineering:** Implementation of structured instructions (System Prompt) with specific meta-rules for handling most common types of mistakes.\n",
        "2.  **RAG System (Knowledge Base):** Construction of a vector-based Knowledge Base from the training dataset to enable **Dynamic Few-Shot Prompting**. This allows the model to take into account labeling patterns from similar historical cases.\n",
        "3.  **Reflection Pattern:** Integration of a secondary \"Critic\" model to audit draft responses. Includes conditional logic to trigger the Critic only when necessary.\n",
        "4.  **External Search Tool:** Incorporation of web search capabilities to fetch missing context for ambiguous entities."
      ],
      "metadata": {
        "id": "SWPlLGwLz53y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Data loading"
      ],
      "metadata": {
        "id": "shNA5Libzi3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZlYErLJ9Rgy"
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas openai pydantic requests tqdm litellm opik scikit-learn sentence-transformers faiss-cpu tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from urllib.parse import urlencode\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def download_file_from_yadisk(public_key: str):\n",
        "    base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
        "    final_url = base_url + urlencode(dict(public_key=public_key))\n",
        "    response = requests.get(final_url)\n",
        "    download_url = response.json()['href']\n",
        "\n",
        "    download_response = requests.get(download_url)\n",
        "    return io.BytesIO(download_response.content)\n",
        "\n",
        "TEST_PUBLIC_LINK = \"https://disk.360.yandex.ru/d/aCpPMD--Yi_y5g\"\n",
        "TRAIN_PUBLIC_LINK = \"https://disk.360.yandex.ru/d/Y4HNAcJh6_cNog\"\n",
        "\n",
        "try:\n",
        "    test_file_content = download_file_from_yadisk(TEST_PUBLIC_LINK)\n",
        "    train_file_content = download_file_from_yadisk(TRAIN_PUBLIC_LINK)\n",
        "\n",
        "    df_test = pd.read_json(test_file_content, lines=True)\n",
        "    df_train = pd.read_json(train_file_content, lines=True)\n",
        "\n",
        "    print(f\"Test dataset loaded. Shape: {df_test.shape}\")\n",
        "    print(f\"Train dataset loaded. Shape: {df_train.shape}\")\n",
        "    print(\"Columns test:\", df_test.columns.tolist())\n",
        "    print(\"Columns train:\", df_train.columns.tolist())\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка загрузки: {e}\")"
      ],
      "metadata": {
        "id": "HDNT_fzN95zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_kb, df_val = train_test_split(df_train, test_size=0.05, random_state=42, stratify=df_train['relevance_new'])"
      ],
      "metadata": {
        "id": "7FtKVZKQTH3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports and configs"
      ],
      "metadata": {
        "id": "8Ni-Z66h0T84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Optional, Dict, Any\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import litellm\n",
        "from litellm import Router\n",
        "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
        "\n",
        "from opik import track\n",
        "from opik.opik_context import get_current_span_data, update_current_span\n",
        "from litellm.integrations.opik.opik import OpikLogger\n",
        "\n",
        "from tavily import TavilyClient\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "os.environ[\"OPIK_API_KEY\"] = \"...\"\n",
        "os.environ[\"OPIK_WORKSPACE\"] = \"default\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"tvly-dev-...\"\n",
        "\n",
        "opik_logger = OpikLogger()\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "API_KEYS = [\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\",\n",
        "    \"sk-or-v1-...\"\n",
        "]\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    \"mimo-v2-flash\": \"openrouter/xiaomi/mimo-v2-flash:free\",\n",
        "    \"gpt-oss-120b\": \"openrouter/openai/gpt-oss-120b:free\",\n",
        "    \"glm-4.5-air\": \"openrouter/z-ai/glm-4.5-air:free\",\n",
        "    \"kimi\": \"openrouter/moonshotai/kimi-k2:free\",\n",
        "    \"deepseek-r1\": \"openrouter/deepseek/deepseek-r1-0528:free\"\n",
        "}\n",
        "\n",
        "model_list = []\n",
        "\n",
        "for alias, model_id in MODELS_CONFIG.items():\n",
        "    for key in API_KEYS:\n",
        "        model_list.append({\n",
        "            \"model_name\": alias,\n",
        "            \"litellm_params\": {\n",
        "                \"model\": model_id,\n",
        "                \"api_key\": key\n",
        "            }\n",
        "        })\n",
        "\n",
        "llm_router = Router(\n",
        "    model_list=model_list,\n",
        "    routing_strategy=\"simple-shuffle\",\n",
        "    timeout=30,\n",
        "    num_retries=2,\n",
        "    allowed_fails=3,\n",
        "    cooldown_time=60,\n",
        ")\n",
        "\n",
        "class BaseResult(BaseModel):\n",
        "    reasoning: str = Field(description=\"Detailed Step-by-step thinking process.\")\n",
        "    score: float = Field(description=\"Relevance score: must be exactly 0.0, 0.1, or 1.0\")\n",
        "\n",
        "    @field_validator('score')\n",
        "    @classmethod\n",
        "    def snap_to_grid(cls, v: int) -> float:\n",
        "        allowed = [0.0, 0.1, 1.0]\n",
        "        closest = min(allowed, key=lambda x: abs(x - v))\n",
        "        return closest\n",
        "\n",
        "class DraftDecision(BaseModel):\n",
        "    reasoning: str = Field(description=\"Analysis of the current information\")\n",
        "    needs_search: bool = Field(description=\"Set to True if critical information is missing\")\n",
        "    search_query: Optional[str] = Field(description=\"Specific search query if needs_search is True\", default=None)\n",
        "    preliminary_score: float = Field(description=\"Score if no search is needed\", default=None)\n",
        "\n",
        "    @field_validator('preliminary_score')\n",
        "    @classmethod\n",
        "    def snap_score(cls, v: Optional[float]) -> Optional[float]:\n",
        "        if v is None: return None\n",
        "        allowed = [0.0, 0.1, 1.0]\n",
        "        return min(allowed, key=lambda x: abs(x - v))\n",
        "\n",
        "class FinalVerdict(BaseModel):\n",
        "    reasoning: str = Field(description=\"Final detailed reasoning\")\n",
        "    score: float = Field(description=\"Final score: 0.0, 0.1, or 1.0\")\n",
        "\n",
        "    @field_validator('score')\n",
        "    @classmethod\n",
        "    def snap_score(cls, v: float) -> float:\n",
        "        allowed = [0.0, 0.1, 1.0]\n",
        "        return min(allowed, key=lambda x: abs(x - v))\n",
        "\n",
        "def format_input_context(row: pd.Series) -> str:\n",
        "    return f\"\"\"\n",
        "USER QUERY: \"{row['Text']}\"\n",
        "\n",
        "OBJECT DETAILS:\n",
        "- Name: {row['name']}\n",
        "- Rubric: {row['normalized_main_rubric_name_ru']}\n",
        "- Address: {row['address']}\n",
        "- Prices: {row['prices_summarized'] if pd.notna(row['prices_summarized']) else \"No data\"}\n",
        "- Reviews: {row['reviews_summarized'] if pd.notna(row['reviews_summarized']) else \"No data\"}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mfqTRZhMjynb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Base solution"
      ],
      "metadata": {
        "id": "aYYmtGOJ72Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_system_prompt():\n",
        "    return \"\"\"\n",
        "Determine the relevance of OBJECT to USER QUERY.\n",
        "\n",
        "VALID OUTPUT VALUES:\n",
        "- 1.0: Relevant. The object provides what the user wants.\n",
        "- 0.0: Irrelevant.\n",
        "- 0.1: Partial/Unsure.\n",
        "\n",
        "Output strictly JSON: {\"score\": float}\n",
        "\"\"\"\n",
        "\n",
        "def get_base_user_prompt(context):\n",
        "    return f\"Analyze this case: {context}\""
      ],
      "metadata": {
        "id": "X2Hvhykrpg_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAgent:\n",
        "    def __init__(self, router: Router, model_alias: str):\n",
        "        self.router = router\n",
        "        self.model_alias = model_alias\n",
        "\n",
        "    def _call_llm(self, messages: List[Dict[str, str]]) -> BaseResult:\n",
        "        try:\n",
        "            response = self.router.completion(\n",
        "                model=self.model_alias,\n",
        "                messages=messages,\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "            )\n",
        "            content = response.choices[0].message.content\n",
        "            return BaseResult.model_validate_json(content)\n",
        "\n",
        "        except (ValidationError, json.JSONDecodeError):\n",
        "            return BaseResult(reasoning=\"Parsing Error\", score=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"LLM Call Error: {e}\")\n",
        "            return BaseResult(reasoning=\"API Error\", score=0.0)\n",
        "\n",
        "    @track(name=\"baseline relevance_prediction\")\n",
        "    def predict(self, row: pd.Series) -> float:\n",
        "        context = format_input_context(row)\n",
        "\n",
        "        system_prompt = get_base_system_prompt()\n",
        "        user_prompt = get_base_user_prompt(context)\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "\n",
        "        result = self._call_llm(messages)\n",
        "\n",
        "        return result.score"
      ],
      "metadata": {
        "id": "sw9hect877cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_base_evaluation(agent, dataset, limit = 50):\n",
        "    sample_df = dataset.sample(n=min(limit, len(dataset)), random_state=42).copy()\n",
        "    y_true = sample_df['relevance_new'].tolist()\n",
        "    y_pred = []\n",
        "\n",
        "    print(f\"Starting evaluation on {len(sample_df)} samples...\")\n",
        "\n",
        "    for _, row in sample_df.iterrows():\n",
        "        prediction = agent.predict(row)\n",
        "        y_pred.append(prediction)\n",
        "\n",
        "    sample_df['predicted_relevance'] = y_pred\n",
        "\n",
        "    y_true = [str(val) for val in y_true]\n",
        "    y_pred = [str(val) for val in y_pred]\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    report = classification_report(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    print(f\"\\nEvaluation Complete.\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    return sample_df"
      ],
      "metadata": {
        "id": "wRn3ER1ywL4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Testing mimo-v2-flash ---\")\n",
        "agent_mimo_base = BaseAgent(llm_router, model_alias=\"mimo-v2-flash\")\n",
        "df_mimo_base = run_base_evaluation(agent_mimo_base, df_val, limit=30)"
      ],
      "metadata": {
        "id": "qEgUl4EnuN63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Best solution"
      ],
      "metadata": {
        "id": "LhUeislA0znC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Knowledge Base"
      ],
      "metadata": {
        "id": "9H6umzfB1XYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeBase:\n",
        "    def __init__(self, dataframe, model_name='intfloat/multilingual-e5-large', index_path=\"kb_index\"):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.model_name = model_name\n",
        "        self.index_path_faiss = f\"{index_path}.faiss\"\n",
        "        self.index_path_meta = f\"{index_path}.pkl\"\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Loading embedding model on {self.device}...\")\n",
        "        self.model = SentenceTransformer(model_name, device=self.device)\n",
        "\n",
        "        self.index = None\n",
        "\n",
        "        if os.path.exists(self.index_path_faiss) and os.path.exists(self.index_path_meta):\n",
        "            self.load()\n",
        "        else:\n",
        "            self.build_index()\n",
        "\n",
        "    def _row_to_text(self, row, is_query):\n",
        "        prefix = \"query: \" if is_query else \"passage: \"\n",
        "        content = f\"User request: {row['Text']} | Name: {row['name']} | Address: {row['address']} | Rubric: {row['normalized_main_rubric_name_ru']}\"\n",
        "        return prefix + content\n",
        "\n",
        "    def build_index(self, batch_size=64):\n",
        "        print(\"Building Vector Index...\")\n",
        "        sentences = self.df.apply(lambda x: self._row_to_text(x, is_query=False), axis=1).tolist()\n",
        "\n",
        "        embeddings = self.model.encode(\n",
        "            sentences,\n",
        "            batch_size=batch_size,\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True\n",
        "        )\n",
        "\n",
        "        d = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(d)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "        print(f\"Index built. Vectors: {self.index.ntotal}\")\n",
        "        self.save()\n",
        "\n",
        "    @track(name=\"vector_search\")\n",
        "    def search(self, row, k=3):\n",
        "        query_text = self._row_to_text(row, is_query=True)\n",
        "        query_vec = self.model.encode([query_text], convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "        distances, indices = self.index.search(query_vec, k + 1)\n",
        "\n",
        "        results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if i >= k: break\n",
        "\n",
        "            match_row = self.df.iloc[idx]\n",
        "\n",
        "            results.append({\n",
        "                \"text\": match_row['Text'],\n",
        "                \"name\": match_row['name'],\n",
        "                \"relevance\": match_row['relevance'],\n",
        "                \"distance\": float(distances[0][i])\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    def save(self):\n",
        "        faiss.write_index(self.index, self.index_path_faiss)\n",
        "        with open(self.index_path_meta, \"wb\") as f:\n",
        "            pickle.dump(self.df, f)\n",
        "\n",
        "    def load(self):\n",
        "        self.index = faiss.read_index(self.index_path_faiss)\n",
        "        with open(self.index_path_meta, \"rb\") as f:\n",
        "            self.df = pickle.load(f)"
      ],
      "metadata": {
        "id": "qnWSTT6qHoBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_folder = \"/content/drive/MyDrive/YandexMaps_Agent_Project\"\n",
        "os.makedirs(drive_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "KFuJXjgxfOwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_path_drive = os.path.join(drive_folder, \"kb_index\")\n",
        "kb = KnowledgeBase(df_kb, index_path=index_path_drive)"
      ],
      "metadata": {
        "id": "CE_qXGN1H45l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Prompts"
      ],
      "metadata": {
        "id": "dD3La_LH1ja-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_system_prompt(is_draft=True) -> str:\n",
        "    specific_instructions = ''\n",
        "    if is_draft:\n",
        "        specific_instructions = \"\"\"\n",
        "5. **UNCERTAINTY & SEARCH:**\n",
        "   - If your analysis leads to a **0.1** score, you MUST set `needs_search` to `true`.\n",
        "   - Formulate a specific `search_query` intended to find exactly that missing piece of info (e.g., \"Menu of Cafe Name, address (short)\", \"Does Gym X have a pool\").\n",
        "   - If the score is definitively 0.0 or 1.0 based on current data, set `needs_search` to `false`.\n",
        "\n",
        "6. **OUTPUT FORMAT:** Return strictly JSON with the following keys:\n",
        "   - `\"reasoning\"`: (string) step-by-step analysis.\n",
        "   - `\"needs_search\"`: (boolean)\n",
        "   - `\"search_query\"`: (string or null)\n",
        "   - `\"preliminary_score\"`: (float) 0.0, 0.1, or 1.0\n",
        "\"\"\"\n",
        "    else:\n",
        "        specific_instructions = \"\"\"\n",
        "5. **OUTPUT FORMAT:** Return strictly JSON with the following keys:\n",
        "   - `\"reasoning\"`: (string) Final verdict explanation, incorporating search results if available.\n",
        "   - `\"score\"`: (float) Final relevance score: exactly 0.0, 0.1, or 1.0.\n",
        "\"\"\"\n",
        "    return f\"\"\"\n",
        "You are a search relevance expert. Your goal is to assess if a map object satisfies a user query.\n",
        "\n",
        "VALID OUTPUT VALUES:\n",
        "- 1.0: Perfect match. The object definitely provides what the user wants.\n",
        "- 0.0: Irrelevant. Wrong category, closed, or completely unrelated.\n",
        "- 0.1: Partial/Unsure. The object might be relevant, but specific IMPORTANT (think about importance) constraints (if they are) from user (price, specific service) are not explicitly confirmed in the object details.\n",
        "\n",
        "IMPORTANT: HANDLING HISTORICAL EXAMPLES\n",
        "You will be provided with \"SIMILAR HISTORICAL CASES\" from the training database.\n",
        "1. Use them to understand the *labeling logic* (e.g., how strict the rules are regarding location or category).\n",
        "2. WARNING: The historical labels might be noisy or based on info not visible in the card (e.g., the place was permanently closed at that time).\n",
        "3. If a historical example contradicts common sense (e.g., perfect match labeled 0.0), prioritize your own analysis of the current object's data.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Think step-by-step. Detect the USER INTENT: Is the user looking for a specific **ITEM/SERVICE** (e.g., \"buy pills\", \"sauna\") or a specific **VENUE TYPE** (e.g., \"Pharmacy\", \"Recreation Base\")?\n",
        "2. **IF USER WANTS A VENUE TYPE (Category Search):**\n",
        "   - **RUBRIC PRIORITY:** The object's `Rubric` must semantically match the requested venue type.\n",
        "   - **MISMATCH PENALTY:** If the user asks for \"Category A\" (e.g., \"Holiday Center\") and the object is \"Category B\" (e.g., \"Sports Camp\"), the score is likely **0.0**, even if they share some features (like saunas or beds).\n",
        "   - *Exception:* Only give 0.1 or 1.0 if Category B is a direct sub-type or synonym of Category A.\n",
        "3. **IF USER WANTS AN ITEM/SERVICE:**\n",
        "   - **CATEGORY LOGIC:** If the user asks for a COMMON item (e.g., \"aspirin\") and the object is a standard provider (Pharmacy), **SCORE IS 1.0**.\n",
        "   - **PARTIAL INVENTORY (The \"Open List\" Rule):**\n",
        "     - Treat 'Prices' and 'Description' as **incomplete examples**, not a full catalog.\n",
        "     - If the object sells \"Bags\" but lists only \"Wallets\", assume it MIGHT sell \"Suitcases\".\n",
        "     - **DO NOT DOWNGRADE TO 0.0** just because a specific item or brand is missing from the text description, unless the category makes it impossible (e.g., asking for \"Suitcase\" in a \"Bakery\").\n",
        "     - **Verdict:** If Category matches but Item/Brand is unconfirmed -> Score is **0.1**.\n",
        "   - Use 0.1 for RARE/SPECIFIC items where availability is truly unknown.\n",
        "4. Pay attention to \"Hard Constraints\" (open now, free wifi) in user request.\n",
        "{specific_instructions}\n",
        "\"\"\"\n",
        "\n",
        "def format_rag_context(examples: list) -> str:\n",
        "    if not examples:\n",
        "        return \"No similar examples found.\"\n",
        "\n",
        "    text = \"\"\n",
        "    for i, ex in enumerate(examples):\n",
        "        text += f\"\"\"\n",
        "[Example {i+1}]\n",
        "User Query: \"{ex['text']}\"\n",
        "Object Name: \"{ex['name']}\"\n",
        "Ground Truth Relevance: {ex['relevance']}\n",
        "(Similarity Score: {ex['distance']:.2f})\n",
        "\"\"\"\n",
        "    return text\n",
        "\n",
        "def get_draft_prompt(context: str, rag_text: str):\n",
        "    return f\"\"\"\n",
        "Analyze this case:\n",
        "{context}\n",
        "\n",
        "---\n",
        "SIMILAR HISTORICAL CASES (For Reference):\n",
        "{rag_text}\n",
        "---\n",
        "\"\"\"\n",
        "\n",
        "def get_critique_prompt(context: str, rag_text: str, search_results: str) -> str:\n",
        "    return f\"\"\"\n",
        "Analyze this case:\n",
        "{context}\n",
        "\n",
        "HISTORICAL REFERENCES (For Reference):\n",
        "{rag_text}\n",
        "\n",
        "EXTERNAL EVIDENCE FROM WEB SEARCH (For Reference):\n",
        "{search_results}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NEDzqFlYzGup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Best agent class"
      ],
      "metadata": {
        "id": "H0K8GyKo1tDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BestAgent:\n",
        "    def __init__(self, router: Router, model_alias: str, kb: KnowledgeBase):\n",
        "        self.router = router\n",
        "        self.model_alias = model_alias\n",
        "        self.kb = kb\n",
        "\n",
        "    @track\n",
        "    def _call_llm(self, messages: List[Dict[str, str]], response_model: Any, step_name: str):\n",
        "        update_current_span(name=step_name)\n",
        "\n",
        "        try:\n",
        "            response = self.router.completion(\n",
        "                model=self.model_alias,\n",
        "                messages=messages,\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "            )\n",
        "            content = response.choices[0].message.content\n",
        "            return response_model.model_validate_json(content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"LLM Call Error in {step_name}: {e}\")\n",
        "            if response_model == DraftDecision:\n",
        "                return DraftDecision(reasoning=\"Error\", needs_search=False, preliminary_score=0.1)\n",
        "            return FinalVerdict(reasoning=\"Error\", score=0.0)\n",
        "\n",
        "    @track(name=\"tavily_search_execution\")\n",
        "    def _execute_search(self, query: str) -> str:\n",
        "        try:\n",
        "            response = tavily_client.search(query, search_depth=\"basic\", max_results=2)\n",
        "            snippets = [r['content'] for r in response.get('results', [])]\n",
        "            return \"\\n\".join(snippets)\n",
        "        except Exception as e:\n",
        "            return f\"Search failed: {e}\"\n",
        "\n",
        "    @track(name=\"relevance_prediction\")\n",
        "    def predict(self, row: pd.Series) -> float:\n",
        "        rag_examples = self.kb.search(row, k=3)\n",
        "        rag_text = format_rag_context(rag_examples)\n",
        "\n",
        "        context = format_input_context(row)\n",
        "        system_prompt_draft = get_system_prompt(is_draft=True)\n",
        "\n",
        "        draft_prompt = get_draft_prompt(context, rag_text)\n",
        "\n",
        "        draft_messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt_draft},\n",
        "            {\"role\": \"user\", \"content\": draft_prompt}\n",
        "        ]\n",
        "\n",
        "        draft_result = self._call_llm(draft_messages, DraftDecision, step_name=\"generate_draft\")\n",
        "\n",
        "        trace_log = {\n",
        "            \"needs_search\": draft_result.needs_search,\n",
        "            \"search_query\": draft_result.search_query,\n",
        "            \"draft_score\": draft_result.preliminary_score,\n",
        "            \"draft_reasoning\": draft_result.reasoning,\n",
        "            \"search_results\": None,\n",
        "            \"final_score\": None,\n",
        "            \"final_reasoning\": None\n",
        "        }\n",
        "\n",
        "        if draft_result.needs_search and draft_result.search_query:\n",
        "            search_results = self._execute_search(draft_result.search_query)\n",
        "            trace_log[\"search_results\"] = search_results\n",
        "\n",
        "            critique_prompt_text = get_critique_prompt(context, rag_text, search_results)\n",
        "\n",
        "            critic_messages = [\n",
        "                {\"role\": \"system\", \"content\": get_system_prompt(is_draft=False)},\n",
        "                {\"role\": \"user\", \"content\": critique_prompt_text}\n",
        "            ]\n",
        "\n",
        "            critic_decision = self._call_llm(critic_messages, FinalVerdict, step_name=\"critique_verification\")\n",
        "\n",
        "            trace_log[\"final_score\"] = critic_decision.score\n",
        "            trace_log[\"final_reasoning\"] = critic_decision.reasoning\n",
        "\n",
        "        else:\n",
        "            score = draft_result.preliminary_score\n",
        "            trace_log[\"final_score\"] = score\n",
        "            trace_log[\"final_reasoning\"] = draft_result.reasoning\n",
        "\n",
        "        return trace_log\n"
      ],
      "metadata": {
        "id": "arY1XaaQzKPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Best agent evaluation\n"
      ],
      "metadata": {
        "id": "J0GjDapM2ccB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_evaluation(agent, dataset, limit=50):\n",
        "    sample_df = dataset.sample(n=min(limit, len(dataset)), random_state=42).copy()\n",
        "\n",
        "    final_preds = []\n",
        "    draft_preds = []\n",
        "    did_search_list = []\n",
        "    final_reasonings = []\n",
        "    draft_reasonings = []\n",
        "\n",
        "    print(f\"Starting evaluation on {len(sample_df)} samples...\")\n",
        "\n",
        "    for _, row in sample_df.iterrows():\n",
        "        trace = agent.predict(row)\n",
        "\n",
        "        f_score = trace.get(\"final_score\")\n",
        "        if f_score is None: f_score = 0.0\n",
        "        final_preds.append(f_score)\n",
        "\n",
        "        d_score = trace.get(\"draft_score\")\n",
        "        if d_score is None: d_score = 0.0\n",
        "        draft_preds.append(d_score)\n",
        "\n",
        "        did_search_list.append(trace.get(\"needs_search\", False))\n",
        "        final_reasonings.append(trace.get(\"final_reasoning\", \"\"))\n",
        "        draft_reasonings.append(trace.get(\"draft_reasoning\", \"\"))\n",
        "\n",
        "    sample_df['draft_prediction'] = draft_preds\n",
        "    sample_df['predicted_relevance'] = final_preds\n",
        "    sample_df['did_search'] = did_search_list\n",
        "    sample_df['draft_reasoning'] = draft_reasonings\n",
        "    sample_df['final_reasoning'] = final_reasonings\n",
        "\n",
        "    y_true_str = sample_df['relevance_new'].astype(float).astype(str).tolist()\n",
        "    y_pred_final_str = [str(float(x)) for x in final_preds]\n",
        "\n",
        "    acc = accuracy_score(y_true_str, y_pred_final_str)\n",
        "    search_rate = np.mean(did_search_list)\n",
        "\n",
        "    print(f\"\\nEvaluation Complete.\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Search Rate: {search_rate:.1%}\")\n",
        "    print(\"Classification Report (Final):\")\n",
        "    print(classification_report(y_true_str, y_pred_final_str, zero_division=0))\n",
        "\n",
        "    cols_to_show = [\n",
        "        'Text', 'name', 'address',\n",
        "        'relevance_new',\n",
        "        'draft_prediction',\n",
        "        'predicted_relevance',\n",
        "        'did_search'\n",
        "    ]\n",
        "\n",
        "    display_df = sample_df[cols_to_show].copy()\n",
        "\n",
        "    def highlight_predictions(row):\n",
        "        gt = float(row['relevance_new'])\n",
        "        draft = float(row['draft_prediction'])\n",
        "        final = float(row['predicted_relevance'])\n",
        "\n",
        "        styles = [''] * len(row)\n",
        "\n",
        "        idx_draft = row.index.get_loc('draft_prediction')\n",
        "        idx_final = row.index.get_loc('predicted_relevance')\n",
        "\n",
        "        green_style = 'background-color: #d4edda; color: #155724'\n",
        "        red_style = 'background-color: #f8d7da; color: #721c24'\n",
        "\n",
        "        styles[idx_draft] = green_style if draft == gt else red_style\n",
        "        styles[idx_final] = green_style if final == gt else red_style\n",
        "\n",
        "        return styles\n",
        "\n",
        "    return display_df.style.apply(highlight_predictions, axis=1)"
      ],
      "metadata": {
        "id": "sTOmXgxYwU3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Эксперимент 1: Тестируем mimo-v2-flash\n",
        "print(\"--- Testing mimo-v2-flash ---\")\n",
        "agent_mimo = BestAgent(llm_router, model_alias=\"mimo-v2-flash\", kb=kb)\n",
        "df_mimo = run_evaluation(agent_mimo, df_val, limit=30)\n",
        "df_mimo"
      ],
      "metadata": {
        "id": "yx584No50IBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Эксперимент 2: Тестируем glm-4.5-air\n",
        "print(\"\\n--- Testing glm-4.5-air ---\")\n",
        "agent_glm = BestAgent(llm_router, model_alias=\"glm-4.5-air\", kb=kb)\n",
        "df_glm = run_evaluation(agent_glm, df_val, limit=30)\n",
        "df_glm"
      ],
      "metadata": {
        "id": "weEauLFv5lXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Эксперимент 3: Тестируем deepseek-r1\n",
        "print(\"\\n--- Testing deepseek-r1 ---\")\n",
        "agent_deepseek = BestAgent(llm_router, model_alias=\"deepseek-r1\", kb=kb)\n",
        "df_deepseek = run_evaluation(agent_deepseek, df_val, limit=30)\n",
        "df_deepseek"
      ],
      "metadata": {
        "id": "SBUQbUCzJVwQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
